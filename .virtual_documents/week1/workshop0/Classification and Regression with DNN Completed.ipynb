











pip install pandas


import pandas as pd



data_path = r"./housepricedata.csv"
df = pd.read_csv(data_path, sep = ',', encoding='utf-8') #wherever resides your data, if .ipnb and .csv are in the same folder no need to worry!
df.head(6)



#print(len(df. columns))
#len(df)
df.shape





dataset = df.values





X = dataset[:,0:10]
Y = dataset[:,10]





pip install scikit-learn


from sklearn import preprocessing


min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)


X_scale  # Objective, target or output variable should not be scaled





from sklearn.model_selection import train_test_split


X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.1)


X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)


print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)





pip install -U tensorflow





pip install protobuf==5.28.*


from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout


# Install NVIDIA Collective Communications Library (NCCL) for CUDA 12
# NCCL enables efficient multi-GPU communication for deep learning frameworks
!pip install nvidia-pyindex && nvidia-nccl-cu12



pip install --extra-index-url https://pypi.nvidia.com tensorflow[and-cuda]





model = Sequential([
    Dense(16, activation='relu', input_shape=(10,)),
    Dropout(0.3),
    Dense(16, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid'),
])





model.compile(optimizer='sgd',
              loss='binary_crossentropy',
              metrics=['accuracy'])





hist = model.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val)) #epochs y bs son hyperparameters





model.evaluate(X_test, Y_test)[1]











pip install matplotlib


import matplotlib.pyplot as plt





plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Train_loss', 'Valid_loss'], loc='upper right')
plt.show()





plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train', 'Valid'], loc='lower right')
plt.show()


# Define the neural network model
model = Sequential([
    Dense(8, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(8, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
hist = model.fit(X_train, Y_train, epochs=40, batch_size=16, validation_data=(X_test, Y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, Y_test)
print(f"Test Accuracy: {accuracy:.4f}")



plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Train_loss', 'Valid_loss'], loc='upper right')
plt.show()


plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train', 'Valid'], loc='lower right')
plt.show()








model_2 = Sequential([
    Dense(1000, activation='relu', input_shape=(10,)),
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1, activation='sigmoid'),
])

# using adaptative moment estimation (adam)

model_2.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
hist_2 = model_2.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))


# Evaluate the model
loss, accuracy = model_2.evaluate(X_test, Y_test)
print(f"Test Accuracy: {accuracy:.4f}")





plt.plot(hist_2.history['loss'])
plt.plot(hist_2.history['val_loss'])
plt.title('Loss for an overfitted model')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()


plt.plot(hist_2.history['accuracy'])
plt.plot(hist_2.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()





from keras.layers import Dropout
from keras import regularizers


model_3 = Sequential([
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),
    Dropout(0.5),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.5),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.5),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.5),
    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),
])


a = 0.01
b = -0.01
c = 0.1
print(a*a)
print(b*b)
print(c*c)


model_3.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
hist_3 = model_3.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))


# Evaluate the model
loss, accuracy = model_3.evaluate(X_test, Y_test)
print(f"Test Accuracy: {accuracy:.4f}")





plt.plot(hist_3.history['loss'])
plt.plot(hist_3.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.ylim(top=1.2, bottom=0)
plt.show()


plt.plot(hist_3.history['accuracy'])
plt.plot(hist_3.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()





model_3v2 = Sequential([
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),
])


model_3v2.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
hist_3v2 = model_3v2.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))


plt.plot(hist_3v2.history['loss'])
plt.plot(hist_3v2.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.ylim(top=1.2, bottom=0)
plt.show()


plt.plot(hist_3v2.history['accuracy'])
plt.plot(hist_3v2.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()














# data analysis
import pandas as pd
import numpy as np

# models
from keras import layers
from keras import preprocessing
from keras import regularizers
from keras.models import Sequential
from keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam   # if keras does not find it

# preprocessing
from sklearn import preprocessing

# Split
from sklearn.model_selection import train_test_split


housePrices=pd.read_excel(r'./kc_house_data_yr.xlsx')
print(len(housePrices))

# preview the data desde la cabeza
housePrices.head()



housePrices.tail()  # previewing the data from the backwards, this is from the tail !


housePrices.shape


len(housePrices.columns)


housePrices.dtypes   # Investigation data types is always good !  object type are strings**


housePrices.info()





# Number of null values per feature
housePrices.isnull().sum()





housePrices.describe().transpose()














!pip install seaborn


# visualization
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

sns.set(style='whitegrid', font_scale=1)

plt.figure(figsize=(13,13))
plt.title('Pearson Correlation Matrix',fontsize=25)
sns.heatmap(housePrices.corr(),linewidths=0.25,vmax=0.7,square=True,cmap='GnBu',linecolor='w',
            annot=True, annot_kws={'size':7}, cbar_kws={'shrink': .7})





price_corr = housePrices.corr()['price'].sort_values(ascending=False)
print(price_corr)





f, axes = plt.subplots(1, 2,figsize=(15,5))
sns.histplot(housePrices['price'], ax=axes[0])
sns.scatterplot(x='price',y='sqft_living', data=housePrices, ax=axes[1])
sns.despine(bottom=True, left=True)
axes[0].set(xlabel='Price in millions (USD)', ylabel='Frequency', title='Price Distribuition')
axes[1].set(xlabel='Price', ylabel='Sqft Living', title='Price vs Sqft Living')
axes[1].yaxis.set_label_position("right")
axes[1].yaxis.tick_right()





sns.set(style="whitegrid", font_scale=1)

f, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=housePrices['bedrooms'],y=housePrices['price'], ax=axes[0])
sns.boxplot(x=housePrices['floors'],y=housePrices['price'], ax=axes[1])
sns.despine(bottom=True, left=True)
axes[0].set(xlabel='Bedrooms', ylabel='Price', title='Bedrooms vs Price Box Plot')
axes[1].set(xlabel='Floors', ylabel='Price', title='Floors vs Price Box Plot')





f, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x=housePrices['waterfront'],y=housePrices['price'], ax=axes[0])
sns.boxplot(x=housePrices['view'],y=housePrices['price'], ax=axes[1])
sns.despine(left=True, bottom=True)
axes[0].set(xlabel='Waterfront', ylabel='Price', title='Waterfront vs Price Box Plot')
axes[1].set(xlabel='View', ylabel='Price', title='View vs Price Box Plot')

f, axe = plt.subplots(1, 1,figsize=(15,5))
sns.boxplot(x=housePrices['grade'],y=housePrices['price'], ax=axe)
sns.despine(left=True, bottom=True)
axe.set(xlabel='Grade', ylabel='Price', title='Grade vs Price Box Plot')


[housePrices['view'].unique().tolist() for view in housePrices.columns]






housePrices = housePrices.drop('id',axis=1)
housePrices = housePrices.drop('zipcode',axis=1)
print(housePrices.columns.values)






housePrices.info()


housePrices['date'] = housePrices['date'].values.astype(str)


housePrices.info()


housePrices['date'] = pd.to_datetime(housePrices['date'])

housePrices['month'] = housePrices['date'].apply(lambda date:date.month)
housePrices['year'] = housePrices['date'].apply(lambda date:date.year)

housePrices = housePrices.drop('date',axis=1)

# Check the new columns
print(housePrices.columns.values)


print('now the dataframe looks like this:')
housePrices.head(5)





f, axes = plt.subplots(1, 2,figsize=(15,5))
sns.boxplot(x='year',y='price',data=housePrices, ax=axes[0])
sns.boxplot(x='month',y='price',data=housePrices, ax=axes[1])
sns.despine(left=True, bottom=True)
axes[0].set(xlabel='Year', ylabel='Price', title='Price by Year Box Plot')
axes[1].set(xlabel='Month', ylabel='Price', title='Price by Month Box Plot')

f, axe = plt.subplots(1, 1,figsize=(15,5))
housePrices.groupby('month').mean()['price'].plot()
sns.despine(left=True, bottom=True)
axe.set(xlabel='Month', ylabel='Price', title='Price Trends')








# Features
X = housePrices.drop('price',axis=1)

# Label
y = housePrices['price']

# Split
X_train, X_valid, y_train, y_valid = train_test_split(X,y,test_size=0.3,random_state=101)


print(X_train.shape)
print(X_valid.shape)
print(y_train.shape)
print(y_valid.shape)





# scaling and train test split
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler()

# fit and transfrom
X_train = scaler.fit_transform(X_train)

# transform for avoiding data leakage
X_valid = scaler.transform(X_valid)

# everything has been scaled between 1 and 0
print('Max: ',X_train.max())
print('Min: ', X_train.min())





model = Sequential()

# input layer
model.add(Dense(19,activation='relu'))

# hidden layers
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))
model.add(Dense(19,activation='relu'))

# output layer
model.add(Dense(1))  # This is the decision layer class or value depending on the need !

model.compile(optimizer='adam',loss='mse') #mean square error





model.fit(x=X_train,y=y_train.values,
          validation_data=(X_valid,y_valid.values),
          batch_size=128,epochs=400)





losses = pd.DataFrame(model.history.history)

plt.figure(figsize=(15,5))
sns.lineplot(data=losses,lw=3)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training Loss per Epoch')
sns.despine()








# evaluation on test data
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score
from sklearn.metrics import classification_report,confusion_matrix


# predictions on the test set
predictions = model.predict(X_valid)

print('MAE: ', mean_absolute_error(y_valid,predictions))
print('MSE: ', mean_squared_error(y_valid,predictions))
print('RMSE: ', np.sqrt(mean_squared_error(y_valid,predictions)))
print('Variance Regression Score: ', explained_variance_score(y_valid,predictions))

print('\n\nDescriptive Statistics:\n',housePrices['price'].describe())





f, axes = plt.subplots(1, 2,figsize=(15,5))

# Our model predictions
plt.scatter(y_valid,predictions)

# Perfect predictions
plt.plot(y_valid,y_valid,'r')

errors = y_valid.values.reshape(6484, 1) - predictions
sns.histplot(errors, ax=axes[0])

sns.despine(left=True, bottom=True)
axes[0].set(xlabel='Error', ylabel='', title='Error Histogram')
axes[1].set(xlabel='Test True Y', ylabel='Model Predictions', title='Model Predictions vs Perfect Fit')





# features of new house
single_house = housePrices.drop('price',axis=1).iloc[0]
print(f'Features of new house:\n {single_house}')

# reshape the numpy array and scale the features
single_house = scaler.transform(single_house.values.reshape(-1, 19))

# run the model and get the price prediction
print('\nPrediction Price:', model.predict(single_house)[0,0])

# original price
print('\nOriginal Price:', housePrices.iloc[0]['price'])





housePrices2 = housePrices[housePrices['price'] <= 3000000]
housePrices2.head(10)









