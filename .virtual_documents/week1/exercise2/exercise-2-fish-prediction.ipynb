


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("./fish_data.csv")
df





df.head()


df.info()


import matplotlib.pyplot as plt
import seaborn as sns

order = df['species'].value_counts().index

plt.figure(figsize=(10,5))
sns.countplot(x='species', data=df, palette='tab20', order=order)
plt.xticks(rotation=45)
plt.title("Number of samples per species (descending)")
plt.show()


sns.histplot(df['length'], kde=True)
plt.show()
sns.histplot(df['weight'], kde=True)
plt.show()


sns.scatterplot(
    x='length', y='weight', hue='species', data=df,
    s=50,
    alpha=0.6
)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.show()








from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

X = df[['length', 'weight', 'w_l_ratio']].values
y = df['species'].values


import numpy as np
print("Before preprocesing")
print("______________________________________________________________")
print(
    f"max value: {np.max(X):.4f}\n"
    f"min value: {np.min(X):.4f}\n"
    f"mean: {np.mean(X):.4f}\n"
    f"std: {np.std(X):.4f}"
)
print("______________________________________________________________")

scaler = StandardScaler()
X_scaled = X #scaler.fit_transform(X)
print("After preprocesing")
print("______________________________________________________________")
print(
    f"max value: {np.max(X_scaled):.4f}\n"
    f"min value: {np.min(X_scaled):.4f}\n"
    f"mean: {np.mean(X_scaled):.4f}\n"
    f"std: {np.std(X_scaled):.4f}"
)
print("______________________________________________________________")


print("Before Encoder")
print(set(y))
le = LabelEncoder()
y_encoded = le.fit_transform(y)
print("______________________________________________")

print("\nAfter Encoder")
print(set(y_encoded))
print("______________________________________________")


X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)





!pip install torch torchvision torchaudio


import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

class FishNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(FishNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        return self.net(x)

input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = len(le.classes_)

model = FishNet(input_dim, hidden_dim, output_dim)








criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


epochs = 50
train_losses = []
test_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        outputs = model(xb)
        loss = criterion(outputs, yb)
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * xb.size(0)
    epoch_train_loss = running_loss / len(train_loader.dataset)
    train_losses.append(epoch_train_loss)

    model.eval()
    running_test_loss = 0
    with torch.no_grad():
        for xb, yb in test_loader:
            outputs = model(xb)
            loss = criterion(outputs, yb)
            running_test_loss += loss.item() * xb.size(0)
    epoch_test_loss = running_test_loss / len(test_loader.dataset)
    test_losses.append(epoch_test_loss)

    print(f"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_train_loss:.4f} - Test Loss: {epoch_test_loss:.4f}")


import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(range(1, epochs+1), train_losses, label='Train Loss', marker='o')
plt.plot(range(1, epochs+1), test_losses, label='Test Loss', marker='s')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Train vs Test Loss')
plt.legend()
plt.grid(True)
plt.show()



from sklearn.metrics import accuracy_score, classification_report

model.eval()
with torch.no_grad():
    y_train_pred = model(X_train_tensor)
    y_train_labels = torch.argmax(y_train_pred, axis=1)

    y_test_pred = model(X_test_tensor)
    y_test_labels = torch.argmax(y_test_pred, axis=1)

train_acc = accuracy_score(y_train, y_train_labels)
print(f"\nTrain Accuracy: {train_acc:.4f}")
print("Classification Report - Train:")
print(classification_report(y_train, y_train_labels, target_names=le.classes_))


test_acc = accuracy_score(y_test, y_test_labels)
print(f"Test Accuracy: {test_acc:.4f}")
print("Classification Report - Test:")
print(classification_report(y_test, y_test_labels, target_names=le.classes_))


from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm_train = confusion_matrix(y_train, y_train_labels)
disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train, display_labels=le.classes_)
disp_train.plot(cmap=plt.cm.Blues, xticks_rotation=45)
plt.title("Confusion Matrix - Train")
plt.show()

cm_test = confusion_matrix(y_test, y_test_labels)
disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test, display_labels=le.classes_)
disp_test.plot(cmap=plt.cm.Oranges, xticks_rotation=45)
plt.title("Confusion Matrix - Test")
plt.show()











import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# ======================
# 1. Load and prepare data
# ======================

if 'w_l_ratio' not in df.columns:
    df['w_l_ratio'] = df['weight'] / (df['length'] + 1e-6)

X = df[['length', 'weight', 'w_l_ratio']].values
y = df['species'].values

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

le = LabelEncoder()
y_encoded = le.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
y_test_tensor = torch.tensor(y_test, dtype=torch.long)

train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)
test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32)

# ======================
# 2. Define model
# ======================
class FishNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(FishNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
    def forward(self, x):
        return self.net(x)

input_dim = X_train.shape[1]
hidden_dim = 64
output_dim = len(le.classes_)

# ======================
# 3. Training function
# ======================
def train_model(epochs):
    model = FishNet(input_dim, hidden_dim, output_dim)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    train_losses, test_losses = [], []
    train_accs, test_accs = [], []
    best_loss = float('inf')
    patience = 10
    counter = 0

    for epoch in range(epochs):
        # Training
        model.train()
        running_loss = 0
        for xb, yb in train_loader:
            optimizer.zero_grad()
            outputs = model(xb)
            loss = criterion(outputs, yb)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * xb.size(0)
        epoch_train_loss = running_loss / len(train_loader.dataset)
        train_losses.append(epoch_train_loss)

        # Evaluation
        model.eval()
        with torch.no_grad():
            train_out = model(X_train_tensor)
            test_out = model(X_test_tensor)

            train_loss = criterion(train_out, y_train_tensor).item()
            test_loss = criterion(test_out, y_test_tensor).item()

            train_preds = torch.argmax(train_out, axis=1)
            test_preds = torch.argmax(test_out, axis=1)

            train_acc = accuracy_score(y_train, train_preds)
            test_acc = accuracy_score(y_test, test_preds)

        test_losses.append(test_loss)
        train_accs.append(train_acc)
        test_accs.append(test_acc)

        # Optional early stopping
        if test_loss < best_loss:
            best_loss = test_loss
            counter = 0
        else:
            counter += 1
            if counter >= patience:
                print(f"Early stopping at epoch {epoch+1}")
                break

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | "
              f"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}")

    return model, train_losses, test_losses, train_accs, test_accs

# ======================
# 4. Run experiments
# ======================
epochs_list = [50, 100, 200]
results = {}

for ep in epochs_list:
    print(f"\n=== Training with {ep} epochs ===")
    model, train_l, test_l, train_a, test_a = train_model(ep)
    results[ep] = {
        "model": model,
        "train_loss": train_l,
        "test_loss": test_l,
        "train_acc": train_a,
        "test_acc": test_a
    }

# ======================
# 5. Plot results
# ======================
plt.figure(figsize=(12,5))
for ep in epochs_list:
    plt.plot(results[ep]["train_loss"], label=f"Train Loss {ep} ep")
    plt.plot(results[ep]["test_loss"], label=f"Test Loss {ep} ep")
plt.xlabel("Epochs"); plt.ylabel("Loss")
plt.legend(); plt.title("Loss Comparison"); plt.show()

plt.figure(figsize=(12,5))
for ep in epochs_list:
    plt.plot(results[ep]["train_acc"], label=f"Train Acc {ep} ep")
    plt.plot(results[ep]["test_acc"], label=f"Test Acc {ep} ep")
plt.xlabel("Epochs"); plt.ylabel("Accuracy")
plt.legend(); plt.title("Accuracy Comparison"); plt.show()

# ======================
# 6. Final report
# ======================
for ep in epochs_list:
    print(f"\nFinal results with {ep} epochs:")
    final_model = results[ep]["model"]
    final_model.eval()
    with torch.no_grad():
        y_test_pred = final_model(X_test_tensor).argmax(axis=1)
    print("Final Accuracy:", accuracy_score(y_test, y_test_pred))
    print(classification_report(y_test, y_test_pred, target_names=le.classes_))









import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

def train_once(lr, epochs=80, weight_decay=0.0, use_scheduler=False):
    """
    Trains a new model with the given learning rate using the already loaded tensors and dataloaders.
    Returns metrics per epoch to analyze speed, stability, and final performance.
    """
    model = FishNet(input_dim, 64, output_dim)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    scheduler = None
    if use_scheduler:
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=False
        )

    train_losses, test_losses = [], []
    train_accs, test_accs = [], []

    for ep in range(epochs):
        # --- training ---
        model.train()
        running = 0.0
        for xb, yb in train_loader:
            optimizer.zero_grad(set_to_none=True)
            out = model(xb)
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()
            running += loss.item() * xb.size(0)

        # Recalculate loss/accuracy on full set for consistency
        model.eval()
        with torch.no_grad():
            out_tr = model(X_train_tensor)
            out_te = model(X_test_tensor)

            tr_loss = criterion(out_tr, y_train_tensor).item()
            te_loss = criterion(out_te, y_test_tensor).item()

            tr_pred = out_tr.argmax(dim=1)
            te_pred = out_te.argmax(dim=1)
            tr_acc = accuracy_score(y_train_tensor.cpu().numpy(), tr_pred.cpu().numpy())
            te_acc = accuracy_score(y_test_tensor.cpu().numpy(), te_pred.cpu().numpy())

        train_losses.append(tr_loss)
        test_losses.append(te_loss)
        train_accs.append(tr_acc)
        test_accs.append(te_acc)

        if scheduler is not None:
            scheduler.step(te_loss)

    return model, train_losses, test_losses, train_accs, test_accs


# =======================
# Learning Rate Experiments
# =======================
lrs = [1e-4, 1e-3, 1e-2]
epochs = 80
weight_decay = 0.0
use_scheduler = False

results = {}
for lr in lrs:
    model_lr, tr_l, te_l, tr_a, te_a = train_once(
        lr=lr, epochs=epochs, weight_decay=weight_decay, use_scheduler=use_scheduler
    )
    results[lr] = {"model": model_lr, "tr_l": tr_l, "te_l": te_l, "tr_a": tr_a, "te_a": te_a}

# =======================
# Comparative Plots
# =======================
plt.figure(figsize=(12,5))
for lr in lrs:
    plt.plot(results[lr]["tr_l"], label=f"Train Loss (LR={lr:.0e})")
    plt.plot(results[lr]["te_l"], label=f"Test Loss (LR={lr:.0e})")
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Loss per Epoch vs Learning Rate")
plt.legend(); plt.grid(True); plt.show()

plt.figure(figsize=(12,5))
for lr in lrs:
    plt.plot(results[lr]["tr_a"], label=f"Train Acc (LR={lr:.0e})")
    plt.plot(results[lr]["te_a"], label=f"Test Acc (LR={lr:.0e})")
plt.xlabel("Epoch"); plt.ylabel("Accuracy"); plt.title("Accuracy per Epoch vs Learning Rate")
plt.legend(); plt.grid(True); plt.show()

# =======================
# Final Summary per LR
# =======================
for lr in lrs:
    tr_l = results[lr]["tr_l"]; te_l = results[lr]["te_l"]
    tr_a = results[lr]["tr_a"]; te_a = results[lr]["te_a"]
    best_epoch = int(torch.tensor(te_l).argmin().item())  # epoch with lowest test loss
    print(f"\nLR={lr:.0e} | Best test loss at epoch {best_epoch+1}/{epochs}: "
          f"{te_l[best_epoch]:.4f} | Test Acc: {te_a[best_epoch]:.4f}")
    print(f"Last epoch -> Test Loss: {te_l[-1]:.4f} | Test Acc: {te_a[-1]:.4f}")









import torch
import torch.nn as nn
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# ----------------------------------------------------
# Model builder: depth (n_layers) and width (hidden_dim)
# ----------------------------------------------------
class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, n_layers=2, p_drop=0.0):
        super().__init__()
        layers = []
        # Input layer
        layers.append(nn.Linear(input_dim, hidden_dim))
        layers.append(nn.ReLU())
        if p_drop > 0: layers.append(nn.Dropout(p_drop))
        # Additional hidden layers
        for _ in range(n_layers - 1):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
            layers.append(nn.ReLU())
            if p_drop > 0: layers.append(nn.Dropout(p_drop))
        # Output layer
        layers.append(nn.Linear(hidden_dim, output_dim))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)

def train_once(model, epochs=80, lr=1e-3, weight_decay=0.0):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

    train_losses, test_losses, train_accs, test_accs = [], [], [], []

    for ep in range(epochs):
        # --- train ---
        model.train()
        for xb, yb in train_loader:
            optimizer.zero_grad(set_to_none=True)
            out = model(xb)
            loss = criterion(out, yb)
            loss.backward()
            optimizer.step()

        # --- evaluate on full sets for consistency ---
        model.eval()
        with torch.no_grad():
            out_tr = model(X_train_tensor)
            out_te = model(X_test_tensor)
            tr_loss = criterion(out_tr, y_train_tensor).item()
            te_loss = criterion(out_te, y_test_tensor).item()
            tr_pred = out_tr.argmax(dim=1)
            te_pred = out_te.argmax(dim=1)
            tr_acc = accuracy_score(y_train_tensor.cpu().numpy(), tr_pred.cpu().numpy())
            te_acc = accuracy_score(y_test_tensor.cpu().numpy(), te_pred.cpu().numpy())

        train_losses.append(tr_loss); test_losses.append(te_loss)
        train_accs.append(tr_acc);    test_accs.append(te_acc)

    # best epoch by validation loss
    best_epoch = int(torch.tensor(test_losses).argmin().item())
    return {
        "model": model,
        "train_losses": train_losses,
        "test_losses": test_losses,
        "train_accs": train_accs,
        "test_accs": test_accs,
        "best_epoch": best_epoch,
        "best_test_loss": test_losses[best_epoch],
        "best_test_acc": test_accs[best_epoch],
        "final_test_loss": test_losses[-1],
        "final_test_acc": test_accs[-1]
    }

# ----------------------------------------------------
# Experiments: vary depth and width
# (uses the same preprocessing and tensors already loaded)
# ----------------------------------------------------
configs = [
    {"name": "small",    "n_layers": 1, "hidden_dim": 32,  "p_drop": 0.0},
    {"name": "base",     "n_layers": 2, "hidden_dim": 64,  "p_drop": 0.0},
    {"name": "wide",     "n_layers": 2, "hidden_dim": 128, "p_drop": 0.0},
    {"name": "deep",     "n_layers": 3, "hidden_dim": 64,  "p_drop": 0.0},
    {"name": "large+",   "n_layers": 3, "hidden_dim": 128, "p_drop": 0.0},
]

results = {}
for cfg in configs:
    print(f"\n=== Training: {cfg['name']} | layers={cfg['n_layers']} | hidden={cfg['hidden_dim']} ===")
    m = MLP(input_dim, cfg["hidden_dim"], output_dim, n_layers=cfg["n_layers"], p_drop=cfg["p_drop"])
    res = train_once(m, epochs=80, lr=1e-3, weight_decay=0.0)  # enable weight_decay if desired
    results[cfg["name"]] = {**cfg, **res}
    print(f"Best epoch: {res['best_epoch']+1}/80 | "
          f"Best Test Loss: {res['best_test_loss']:.4f} | Best Test Acc: {res['best_test_acc']:.4f} | "
          f"Final Test Acc: {res['final_test_acc']:.4f}")

# ----------------------------------------------------
# (Optional) compact summary
# ----------------------------------------------------
print("\nSummary by architecture:")
for name, r in results.items():
    print(f"- {name:8s} | layers={r['n_layers']} | hidden={r['hidden_dim']} | "
          f"best_acc={r['best_test_acc']:.4f} (ep {r['best_epoch']+1}) | "
          f"final_acc={r['final_test_acc']:.4f} | "
          f"gap(train-test, final)={r['train_accs'][-1]-r['final_test_acc']:.4f}")




