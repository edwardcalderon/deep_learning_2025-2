{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Hyperparameter Tuning (3, 5, and 8 trials)\n",
    "\n",
    "This notebook runs Keras Tuner searches using the helper functions defined in `hyperparameter_tuning.py` and reports a results table for each case (3, 5, and 8 trials), as requested.\n",
    "\n",
    "- It automatically detects whether the task is a regression or classification based on the dataset in `week1/workshop0/housepricedata.csv` (or other fallback paths handled by `resolve_dataset_path()`).\n",
    "- For each trial count, it records the test metrics and best hyperparameters.\n",
    "- It also plots the training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: ensure dependencies are installed (uncomment to use)\n",
    "# %pip install -q keras_tuner tensorflow scikit-learn seaborn matplotlib pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "import pandas as pd\n",
    "# Make sure we can import from the current directory where hyperparameter_tuning.py resides\n",
    "nb_dir = os.path.dirname(os.path.abspath("__file__")) if "__file__" in globals() else os.getcwd()\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import importlib\n",
    "htt = importlib.import_module('hyperparameter_tuning')\n",
    "\n",
    "# Load data\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, task_type, n_features = htt.load_and_preprocess_data()\n",
    "task_type, n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "results = {}\n",
    "trial_list = [3, 5, 8]\n",
    "\n",
    "for trials in trial_list:\n",
    "    print(f"\n{'='*60}\nRunning tuning with {trials} trials\n{'='*60}")\n",
    "    model, history, test_metrics, best_hps = htt.run_tuning(\n",
    "        X_train_scaled, y_train, X_test_scaled, y_test, task_type, n_features, trials\n",
    "    )\n",
    "    results[trials] = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'best_hps': best_hps,\n",
    "        'history': history,\n",
    "    }\n",
    "    # Per-trials table\n",
    "    metrics_row = {'trials': trials, **test_metrics}\n",
    "    per_df = pd.DataFrame([metrics_row])\n",
    "    display(Markdown(f"### Results table for {trials} trials"))\n",
    "    display(per_df)\n",
    "    # Show best hyperparameters in a readable JSON block\n",
    "    hp_dict = results[trials]['best_hps'].values if hasattr(results[trials]['best_hps'], 'values') else {}\n",
    "    display(Markdown("**Best Hyperparameters:**"))\n",
    "    print(json.dumps(hp_dict, indent=2))\n",
    "    # Plot training history\n",
    "    htt.plot_training_history(history, f'Training History ({trials} Trials)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table across all trial counts\n",
    "rows = []\n",
    "for t, r in results.items():\n",
    "    row = {'trials': t, **r['test_metrics']}\n",
    "    rows.append(row)\n",
    "summary_df = pd.DataFrame(rows).sort_values('trials').reset_index(drop=True)\n",
    "display(Markdown('## Summary across 3, 5, and 8 trials'))\n",
    "display(summary_df)\n",
    "\n",
    "# Optionally, save the summary table next to this notebook\n",
    "out_path = os.path.join(nb_dir, f\"tuning_summary_{task_type}.csv\")\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "print('Saved summary table to:', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
